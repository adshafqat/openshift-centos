

mkdir -p /etc/origin/master/
vi /etc/origin/master/htpasswd



FAILED - RETRYING: Verify that the console is running

I had openshift_web_console_nodeselector={"region":"infra”} in my inventory file, which I needed to remove. 

To check if this fix is working
ansible-playbook -i inventory.ini playbooks/openshift-web-console/config.yml





Cloning "https://github.com/openshift/openshift-jee-sample.git " ...

error: fatal: unable to access 'https://github.com/openshift/openshift-jee-sample.git/':  Failed connect to github.com:443; Connection refused

Modify resolve.conf in worker nodes. Remove the 172.16.8.26.xip.io from search string

[adeelshafqat@worker2 ~]$ sudo su
[root@worker2 adeelshafqat]# vi /etc/resolv.conf 

# nameserver updated by /etc/NetworkManager/dispatcher.d/99-origin-dns.sh
# Generated by NetworkManager
search cluster.local xecuobx3yi1ujh100formnjf2h.zx.internal.cloudapp.net 172.16.8.26.xip.io
nameserver 172.16.8.26

correct file

[root@worker2 adeelshafqat]# vi /etc/resolv.conf 

# nameserver updated by /etc/NetworkManager/dispatcher.d/99-origin-dns.sh
# Generated by NetworkManager
search cluster.local xecuobx3yi1ujh100formnjf2h.zx.internal.cloudapp.net
nameserver 172.16.8.26




Docker Registry and Router failed scheduling 
 
oc adm manage-node master.172.16.8.15.xip.io --schedulable=true
oc label node master.172.16.8.15.xip.io node-role.kubernetes.io/infra=true

oc adm manage-node worker1.172.16.8.17.xip.io --schedulable=true

oc label node worker1.172.16.8.17.xip.io node-role.kubernetes.io/infra=true




TASK [openshift_cluster_monitoring_operator : Wait for the ServiceMonitor CRD to be created] *********************************************************************************************************
FAILED - RETRYING: Wait for the ServiceMonitor CRD to be created (30 retries left).
FAILED - RETRYING: Wait for the ServiceMonitor CRD to be created (29 retries left).
FAILED - RETRYING: Wait for the ServiceMonitor CRD to be created (28 retries left).
FAILED - RETRYING: Wait for the ServiceMonitor CRD to be created (27 retries left)


In /etc/sysconfig/network-scripts/ifcfg-eth0 (CentOS)
There is a flag NM_CONTROLLED=no
Set it to yes in all nodes, reboot all nodes and run the Ansible script again. 
 
 
 
 
connect: connection refused
 
root@master openshift-centos]# oc login -u admin -p admin https://console.master.172.16.8.15.xip.io:8443/
error: dial tcp 172.16.8.15:8443: connect: connection refused - verify you have provided the correct host and port and that the server is currently running.
[root@master openshift-centos]# master-restart api
2
[root@master openshift-centos]# master-restart controllers
2
[root@master openshift-centos]# 






failed: [master.172.16.8.15.xip.io] (item=etcd) => {"attempts": 60, "changed": false, "item": "etcd", "msg": {"cmd": "/usr/bin/oc get pod master-etcd-master.172.16.8.15.xip.io -o json -n kube-system", "results": [{}], "returncode": 1, "stderr": "The connection to the server openshift-3-11-master:8443 was refused - did you specify the right host or port?\n", "stdout": ""}}
FAILED - RETRYING: Wait for control plane pods to appear (60 retries left).
FAILED - RETRYING: Wait for control plane pods to appear (59 retries left).
FAILED - RETRYING: Wait for control plane pods to appear (58 retries left).
FAILED - RETRYING: Wait for control plane pods to appear (57 retries left).
FAILED - RETRYING: Wait for control plane pods to appear (56


add this to [OSEv3:vars]
# Configure nodeIP in the node config
# This is needed in cases where node traffic is desired to go over an
# interface other than the default network interface.
openshift_set_node_ip=true
then add openshift_ip=x.x.x.x to each node
[nodes]
master.rkdomain.test openshift_ip=x.x.x.x openshift_node_group_name='node-config-master'
nodeone.rkdomain.test openshift_ip=x.x.x.x openshift_node_group_name='node-config-compute'
infranode.rkdomain.test openshift_ip=x.x.x.x openshift_node_group_name='node-config-infra'
 
Uninstall, make sure FDNQ (Full qualified domain name. Check domain name settings)) in host file and retry




Issue:

the following error information was pulled from the glusterfs log to help diagnose this issue: 
[2019-08-04 03:03:57.028565] E [MSGID: 100026] [glusterfsd.c:2351:glusterfs_process_volfp] 0-: failed to construct the graph

That means Gluster client and server has different version.

Go to one of the Gluster server and run this command

gluster volume set $volname ctime off
For eg: gluster volume set dist-rep-vol ctime off


Issue:

If your pod can list file but not create files in Gluster server and giving permission error

Go to the master node

sh-4.2$ id root
uid=0(root) gid=0(root) groups=0(root)

oc edit ns my-project

change id value, allow ids from 0

openshift.io/sa.scc.uid-range: 0/10000

Restart application which is using pv (not openshift cluster)




 




